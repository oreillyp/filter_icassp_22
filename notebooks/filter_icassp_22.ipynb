{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe3GJdOqMU0j"
      },
      "source": [
        "<h1 align=\"center\">Inconspicuous and Effective Over-the-Air Adversarial Examples via Adaptive Filtering</h1>\n",
        "<h4 align=\"center\"><i>ICASSP</i> '22</h4>\n",
        "\n",
        "<div align=\"center\">\n",
        "<h4>\n",
        "    <a href=\"https://interactiveaudiolab.github.io/assets/papers/oreilly_awasthi_vijayaraghavan_pardo_2021.pdf\">preprint</a> •\n",
        "        <a href=\"https://interactiveaudiolab.github.io/project/audio-adversarial-examples.html\">website</a> • \n",
        "        <a href=\"https://interactiveaudiolab.github.io/demos/audio-adversarial-examples.html\">audio</a>\n",
        "    </h4>\n",
        "    <p>\n",
        "    by <em>Patrick O'Reilly, Pranjal Awasthi, Aravindan Vijayaragavan, Bryan Pardo</em>\n",
        "    </p>\n",
        "</div>\n",
        "<p align=\"center\"><img src=\"https://interactiveaudiolab.github.io/assets/images/projects/filters.png\" width=\"400\"/></p>\n",
        "    \n",
        "    \n",
        "This notebook and the corresponding repository contain code for the proposed time-varying filter attack and baseline frequency masking attack against a speaker-verification system. After cloning the repository, hyperparameters can be found in `src/constants.py`. Some values differ from the paper to allow for shorter runtimes. To match the experiments in the paper, set:\n",
        "    \n",
        "| Variable | Value | Description |\n",
        "|---|---|---|\n",
        "| `SR` | 16000 | audio sample rate |\n",
        "| `SIG_LEN` | `4.0` | length, in seconds, to which audio signals are padded / trimmed |\n",
        "| `MAX_ITER` | `8000` | number of optimization iterations for each attack |\n",
        "| `N_PER_CLASS` | `10` | number of instances per class on which to perform attacks |\n",
        "| `EOT_ITER` | `1` | frequency of expectation-over-transformation parameter resampling; by default, sample new simulation parameters every iteration |\n",
        "| `N_EVALS` | `2000` | number of random simulations under which to evaluate final generated attacks |\n",
        "| `N_SEGMENTS` | `0` | for speaker verification model, number of fixed-length segments extracted from each utterance to compute embeddings; setting to `0` uses entire utterance to compute a single embedding. Larger values result in only slightly more robust speaker verification models, at the expense of slower prediction |\n",
        "| `DISTANCE_FN` | `'cosine'` | embedding-space distance function for speaker verification model |\n",
        "| `THRESHOLD` | `0.5846` | embedding-space decision threshold for speaker verification model; default value set to EER threshold |\n",
        "| `CONFIDENCE` | `0.5` | for adversarial loss computation, margin by which spoofed audio must fall under verification threshold; used to encourage strong, high-confidence attacks |\n",
        "\n",
        "__Note that these hyperparameters may result in very long runtimes; therefore, it is recomended to try with the default values first.__ Likewise, it is recommended you run this notebook in a CUDA-enabled environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwXUGzzhv-8d"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSg0EjTdiRYQ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/oreillyp/filter_icassp_22.git\n",
        "\n",
        "%cd filter_icassp_22/\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!chmod u+x scripts/download/download_librispeech.sh\n",
        "!chmod u+x scripts/download/download_rir_noise.sh\n",
        "\n",
        "!./scripts/download/download_librispeech.sh\n",
        "!./scripts/download/download_rir_noise.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdO5Y_bQp9dA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from src.constants import *\n",
        "from src.pipelines import Pipeline\n",
        "from src.models import SpeakerVerificationModel, ResNetSE34V2\n",
        "from src.simulation import *\n",
        "from src.preprocess import *\n",
        "from src.loss import *\n",
        "from src.data import LibriSpeechDataset\n",
        "from src.attacks import FilterAttack, FrequencyMaskingAttack\n",
        "from src.writer import Writer\n",
        "from src.utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVwWF113rcoD"
      },
      "source": [
        "## Pipeline\n",
        "\n",
        "We wrap over-the-air simulation, preprocessing, and prediction into a single end-to-end differentiable `Pipeline` to simplify attack optimization. The simulation parameters used in the paper experiments are reproduced below.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://interactiveaudiolab.github.io/demos/images/adaptive_filter/system_diagram.png\" width=\"700\"/></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71qIxIkHreoN"
      },
      "outputs": [],
      "source": [
        "# initialize differentiable over-the-air simulation\n",
        "simulation = Simulation(\n",
        "    Offset(length=[-.15, .15]),\n",
        "    Noise(type='gaussian', snr=[30.0, 40.0]),\n",
        "    Bandpass(low=400, high=6000),\n",
        "    Noise(type='environmental', noise_dir=DATA_DIR / \"noise\" / \"room\", snr=[-5.0, 10.0]),\n",
        "    Reverb(rir_dir=DATA_DIR / \"rir\" / \"real\")\n",
        ")\n",
        "\n",
        "# initialize differentiable preprocessing\n",
        "preprocessor = Preprocessor(\n",
        "    Normalize(),\n",
        "    # VAD(),\n",
        "    PreEmphasis()\n",
        ")\n",
        "\n",
        "# initialize speaker-verification model and load pretrained weights\n",
        "model = SpeakerVerificationModel(ResNetSE34V2(), n_segments=N_SEGMENTS, threshold=THRESHOLD)\n",
        "model.load_weights(MODEL_PATH)\n",
        "\n",
        "# wrap everything into a Pipeline instance\n",
        "pipeline = Pipeline(\n",
        "    model=model,\n",
        "    simulation=simulation,\n",
        "    preprocessor=preprocessor,\n",
        "    device=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Vd8eZLtvVP"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We use the LibriSpeech `test-clean` subset for evaluating attacks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9En-OQFltuPp"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "dataset = LibriSpeechDataset()\n",
        "\n",
        "# set random seed\n",
        "set_random_seed(RAND_SEED)\n",
        "\n",
        "# select subset of data to evaluate attacks\n",
        "x, y_orig, y = stratified_sample(\n",
        "    data=dataset,\n",
        "    n_per_class=N_PER_CLASS,  # starting audio files drawn per class\n",
        "    target=TARGET_CLASS,  # attack target class (exclude inputs from target class)\n",
        "    exclude=EXCLUDE_CLASS,  # exclude inputs from given classes as well\n",
        ")\n",
        "\n",
        "# if no target is given, randomly assign targets (excluding ground truth)\n",
        "if TARGET_CLASS is None:\n",
        "    y = select_random_targets(y_orig, n_per_class=N_PER_CLASS)\n",
        "\n",
        "y_idx = y.clone()\n",
        "\n",
        "# for each input, select a corresponding utterance of the target class and\n",
        "# construct an embedding (or set of embeddings) to serve as a target\n",
        "target_embeddings = []\n",
        "target_audio = []  # save target audio for eventual evaluation\n",
        "for spkr_idx in y:\n",
        "\n",
        "    all_spkr = dataset.tx[dataset.ty == spkr_idx]\n",
        "    x_spkr = all_spkr[torch.randperm(len(all_spkr))][0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        target_audio.append(x_spkr)\n",
        "        target_embeddings.append(\n",
        "            pipeline.model(x_spkr.to(DEVICE))  # omit simulation, defenses\n",
        "        )\n",
        "\n",
        "y = torch.cat(target_embeddings, dim=0)\n",
        "\n",
        "# fold corresponding embeddings together (n_utterances, n_segments, sig_len)\n",
        "y = y.reshape(x.shape[0], max(N_SEGMENTS, 1), -1)\n",
        "\n",
        "# move data to device\n",
        "x, y_orig, y = x.to(DEVICE), y_orig.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "# determine whether any input-target pairs already fall under threshold\n",
        "with torch.no_grad():\n",
        "    match = pipeline.model.match_predict(\n",
        "        pipeline.model(x), y\n",
        "    ) * 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI3zvZtDDR3j"
      },
      "outputs": [],
      "source": [
        "# demonstration: randomized over-the-air simulation\n",
        "x_example, _  = dataset[0]\n",
        "x_example = x_example.to(pipeline.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    print(\"clean audio:\")\n",
        "    play_audio(x_example)\n",
        "\n",
        "    print(\"simulated audio:\")\n",
        "    play_audio(pipeline.simulation(x_example))\n",
        "\n",
        "    pipeline.sample_params()\n",
        "    print(\"simulated audio:\")\n",
        "    play_audio(pipeline.simulation(x_example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B8AlYV2Qp8X"
      },
      "source": [
        "## Attacks\n",
        "\n",
        "You may need to refresh the TensorBoard dashboard below after running the attack code in order for logs to populate. Logs should include scalars (losses and success rates), images (spectrograms, waveforms, and attack parameters), and audio (over-the-line and simulated over-the-air benign and adversarial recordings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiuDxvngstCR"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs --port=6006 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYCtQwWzDR3k"
      },
      "outputs": [],
      "source": [
        "# initialize logger\n",
        "writer = Writer(\n",
        "    root_dir=RUNS_DIR,\n",
        "    name='speaker_recognition_attack',\n",
        "    use_tb=True,\n",
        "    tb_log_iter=LOG_ITER\n",
        ")\n",
        "writer.log_info(f'Using device {DEVICE}')\n",
        "writer.log_info(f'\"Spoofing\" success rate for unperturbed audio: {match.mean().item():0.3f}')\n",
        "\n",
        "# initialize adversarial loss\n",
        "adv_loss = SpeakerEmbeddingLoss(targeted=True,\n",
        "                                reduction=None,\n",
        "                                confidence=CONFIDENCE,\n",
        "                                distance_fn=DISTANCE_FN,\n",
        "                                n_segments=N_SEGMENTS,\n",
        "                                threshold=THRESHOLD)\n",
        "\n",
        "# initialize auxiliary loss\n",
        "aux_loss = MFCCCosineLoss(reduction=None, n_mfcc=128, n_mels=128)\n",
        "\n",
        "# initialize proposed attack\n",
        "flt = FilterAttack(\n",
        "    n_bands=128,\n",
        "    block_size=1024,\n",
        "    pipeline=pipeline,\n",
        "    class_loss=adv_loss,\n",
        "    aux_loss=aux_loss,\n",
        "    max_iter=MAX_ITER,\n",
        "    eot_iter=EOT_ITER,\n",
        "    opt='adam',\n",
        "    lr=0.005,\n",
        "    eps=40.0,  # 35.0 \n",
        "    batch_size=BATCH_SIZE,\n",
        "    mode='selective',\n",
        "    projection_norm=2,\n",
        "    rand_evals=N_EVALS,\n",
        "    k=None,\n",
        "    writer=writer\n",
        ")\n",
        "\n",
        "# initialize baseline frequency-masking attack\n",
        "qin = FrequencyMaskingAttack(\n",
        "        pipeline=pipeline,\n",
        "        class_loss=adv_loss,\n",
        "        max_iter_1=MAX_ITER // 4,\n",
        "        max_iter_2=3 * MAX_ITER // 4,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        opt_1='adam',\n",
        "        opt_2='adam',\n",
        "        alpha=5e-3,  # 5e-4\n",
        "        eps=0.06,\n",
        "        lr_2=1e-4,  #1e-3\n",
        "        eot_iter=EOT_ITER,\n",
        "        rand_evals=N_EVALS,\n",
        "        writer=writer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9ltdgoc7Be9"
      },
      "source": [
        "Finally, we can run each attack. We evaluate generated audio over a further set of randomized acoustic simulations to obtain an estimate of real-world over-the-air performance. At the default setting of `LOG_ITER = 200`, new logs should populate roughly every 10 minutes in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO0SHt8I7Hg-"
      },
      "outputs": [],
      "source": [
        "# time execution\n",
        "st_time = time.time()\n",
        "\n",
        "# set random seed\n",
        "set_random_seed(RAND_SEED)\n",
        "\n",
        "# perform proposed attack\n",
        "adv_x_flt, success_flt = flt.attack(\n",
        "    x,\n",
        "    y,\n",
        ")\n",
        "\n",
        "# set random seed\n",
        "set_random_seed(RAND_SEED)\n",
        "\n",
        "# perform baseline attack\n",
        "adv_x_qin, success_qin = qin.attack(\n",
        "    x,\n",
        "    y,\n",
        ")\n",
        "\n",
        "ed_time = time.time()\n",
        "elapsed = ed_time - st_time\n",
        "writer.log_info(f'time elapsed (s): {elapsed}')\n",
        "\n",
        "writer.log_info(f'\"Spoofing\" success rate for proposed attack: {success_flt.mean().item():0.3f}')\n",
        "writer.log_info(f'\"Spoofing\" success rate for baseline attack: {success_qin.mean().item():0.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_488qJo_x9LM"
      },
      "source": [
        "We'll save all benign and attack audio and prepare a table of results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgj_E6B8DR3m"
      },
      "outputs": [],
      "source": [
        "# create table: success, detection rates\n",
        "table = pd.DataFrame()\n",
        "table['success_baseline'] = tensor_to_np(success_qin).flatten()\n",
        "table['success_proposed'] = tensor_to_np(success_flt).flatten()\n",
        "\n",
        "# log ground truth and selected targets\n",
        "table['ground_truth'] = tensor_to_np(y_orig).flatten()\n",
        "table['target'] = tensor_to_np(y_idx).flatten()\n",
        "\n",
        "# log descriptive audio filenames\n",
        "table['audio_reference'] = [f'reference_{idx}.wav' for idx in range(len(x))]\n",
        "table['audio_baseline'] = [f'baseline_{idx}.wav' for idx in range(len(x))]\n",
        "table['audio_proposed'] = [f'proposed_{idx}.wav' for idx in range(len(x))]\n",
        "table['audio_target'] = [f'target_{idx}.wav' for idx in range(len(x))]\n",
        "\n",
        "# log perturbation norms\n",
        "table['L2_baseline'] = tensor_to_np(\n",
        "    (adv_x_qin - x).norm(p=2, dim=-1).reshape(-1)\n",
        ").flatten()\n",
        "table['L2_proposed'] = tensor_to_np(\n",
        "    (adv_x_flt - x).norm(p=2, dim=-1).reshape(-1)\n",
        ").flatten()\n",
        "table['Linf_baseline'] = tensor_to_np(\n",
        "    (adv_x_qin - x).norm(p=float(\"inf\"), dim=-1).reshape(-1)\n",
        ").flatten()\n",
        "table['Linf_proposed'] = tensor_to_np(\n",
        "    (adv_x_flt - x).norm(p=float(\"inf\"), dim=-1).reshape(-1)\n",
        ").flatten()\n",
        "\n",
        "# save complete results\n",
        "table.to_csv(Path(writer.run_dir) / 'results.csv')\n",
        "\n",
        "# log overall statistics\n",
        "#  1. overall success rate for baseline and proposed attacks\n",
        "#  2. percentage of attacks in which proposed has success rate greater than\n",
        "#     or equal to that of baseline\n",
        "#  3. average perturbation norm of each attack\n",
        "writer.log_info(f'baseline success rate: {table[\"success_baseline\"].mean() :.3f}')\n",
        "writer.log_info(f'proposed success rate: {table[\"success_proposed\"].mean() :.3f}')\n",
        "\n",
        "# save all audio\n",
        "audio_dir = Path(writer.run_dir) / 'audio'\n",
        "ensure_dir(audio_dir)\n",
        "\n",
        "pbar = tqdm(range(len(table)))\n",
        "for i in pbar:\n",
        "    # save original audio\n",
        "    audio = table.iloc[i]['audio_reference']\n",
        "    audio_idx = table.index.values[i]\n",
        "    pbar.set_description(f'saving audio {audio}')\n",
        "    torchaudio.save(audio_dir / audio,\n",
        "                    x[audio_idx].reshape(1, -1).detach().cpu(),\n",
        "                    sample_rate=SR)\n",
        "\n",
        "    # save baseline attack audio\n",
        "    audio = table.iloc[i]['audio_baseline']\n",
        "    audio_idx = table.index.values[i]\n",
        "    pbar.set_description(f'saving audio {audio}')\n",
        "    torchaudio.save(audio_dir / audio,\n",
        "                    adv_x_qin[audio_idx].reshape(1, -1).detach().cpu(),\n",
        "                    sample_rate=SR)\n",
        "\n",
        "    # save proposed attack audio\n",
        "    audio = table.iloc[i]['audio_proposed']\n",
        "    audio_idx = table.index.values[i]\n",
        "    pbar.set_description(f'saving audio {audio}')\n",
        "    torchaudio.save(audio_dir / audio,\n",
        "                    adv_x_flt[audio_idx].reshape(1, -1).detach().cpu(),\n",
        "                    sample_rate=SR)\n",
        "\n",
        "    # save proposed target audio\n",
        "    audio = table.iloc[i]['audio_target']\n",
        "    audio_idx = table.index.values[i]\n",
        "    pbar.set_description(f'saving audio {audio}')\n",
        "    torchaudio.save(audio_dir / audio,\n",
        "                    target_audio[audio_idx].reshape(1, -1).detach().cpu(),\n",
        "                    sample_rate=SR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x2zqyL1764T"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlXF57jf79Tp"
      },
      "outputs": [],
      "source": [
        "table"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ICASSP_Filter_Attack.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "filter_icassp_22",
      "language": "python",
      "name": "filter_icassp_22"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}